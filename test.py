#-*- coding:utf-8 -*-

"""
Author:xunying/Jasmine
Data:17-12-27
Time:上午9:29
"""

# (1,A)=2,(1,D)=3

import pandas as pd
import numpy as np

def get_matrixM(path):
    res = {}
    user_list = []
    item_list = []
    with open(path) as f: # B46_154611_chengji.csv
        line = f.readline()
        while line:
            eles = line.split(',')
            if eles[0] not in res.keys():
                res[eles[0]] = {}
                user_list.append(eles[0])
            if eles[1] not in item_list:
                item_list.append(eles[1])
            res[eles[0]][eles[1]] = int(float(eles[2])/10)
            line = f.readline()

    data=pd.DataFrame(res).T # 成绩矩阵，行表示用户，列表示课程
    M,N=data.shape
    data=np.array(data)
    matrix_all=[] # 存储１－１０个级别对应的0/1矩阵
    for i in range(10):
        matrix=np.zeros([M,N])
        matrix_all.append(matrix)
    for row in range(M):
        for cow in range(N):
            if not np.isnan(data[row][cow]):
                matrix_all[int(data[row][cow]-1)][row][cow]=1  #在对应值的对应级别上修改矩阵

    print(data)
    return matrix_all,data
import scipy.sparse as sp
def naozhewan():
    data=np.arange(1,6)
    train_edges_r=[0,1,2,3,4]
    train_edges_c=[4,3,2,1,0]
    tt=sp.csr_matrix((data, (train_edges_r, train_edges_c)), shape=[5,5])
    li=np.array([[1,3],[4,0]])

from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score,auc,roc_curve

def confusion():
    labels_all = [2, 2, 3, 3, 5]
    preds_all = [2, 2, 4, 3, np.NaN]
    # pre_sore=auc(labels_all,preds_all)
    M = confusion_matrix(labels_all, preds_all)
    print (M)
    n = len(M)
    precision=[]
    recall=[]
    for i in range(len(M[0])):
        rowsum, colsum = sum(M[i]), sum(M[r][i] for r in range(n))
        try:
            print((M[i][i] / float(colsum)))
            print ((M[i][i] / float(rowsum)))
            precision.append((M[i][i] / float(colsum)))
            recall.append((M[i][i] / float(rowsum)))
        except ZeroDivisionError:
            precision.append(0)
            recall.append(0)
    print('pre:{0},recall:{1}'.format(np.mean([i for i in precision if not np.isnan(i)]),np.mean([i for i in recall if not np.isnan(i)])))
    print('=========')
    return np.mean(precision),np.mean(recall)
import sklearn

def practice():
    list=np.arange(1,25).reshape(2,3,-1)
    print(list)
    print('sum:')
    # print(list.sum(axis=0))
    # print(list.sum(axis=1))
    # print(list.sum(axis=2))
    print(np.eye(3))
import matplotlib.pyplot as plt
def plot():

    x=np.linspace(-np.pi,np.pi,256)
    c,s=np.cos(x),np.sin(x)
    plt.plot(x,c,color='red',linestyle='-',label='COS',alpha=0.5)
    plt.plot(x,s,label='SIN')
    ax=plt.gca()
    ax.spines['right'].set_color('none')
    ax.spines['top'].set_color('none')
    ax.spines['left'].set_position(('data',0))
    ax.spines['bottom'].set_position(('data', 0))
    plt.title('sin,consine')
    plt.show()

def time_loss_plot():
    # ltime_list=[0.5195510387420654, 0.26604199409484863, 0.25936412811279297, 0.25212597846984863, 0.26233410835266113, 0.23587298393249512]
    loss_list=[3.5358486441753141, 3.063377833111296, 3.2338430263412072, 3.2418990050684759, 3.1899635311689032, 3.3748882083028811, 3.5035106528728823, 3.7431308712110596, 3.1469289032443317, 3.2629219048032505, 3.0461264924308367, 3.2389504115609919, 3.4038069385144785, 3.6757061779209175, 3.4723549947963273, 3.3153992122335878, 2.8081541453837144, 2.5624503216054344, 2.9439464098570851, 3.323315264464088, 3.2526942828594172, 3.0852747595553192, 2.9972501756041448, 2.9441259087201792, 2.7868720105567997, 2.7757995413364123, 2.7589345546315229, 2.930331966475006, 2.8002173931177663, 2.9550628013050675, 2.9521039317467723, 2.930142687616506, 2.5711410629117313, 2.4860559964167956, 2.8595335972682232, 2.8275768939520929, 2.787924350201604, 2.6821239269035457, 2.6706722771273039, 2.6400586891280753, 2.6680082576380837, 2.6918777766942039, 2.9393290354631847, 2.8804931673150813, 2.6725372797530462, 2.7217194568219472, 2.5918433106150784, 2.5212428854175357, 2.6927092359062623, 2.8181732584357326, 2.6536148698868809, 2.6156624722288773, 2.6535558935480887, 2.6355662603337957, 2.6048643616668867, 2.6942560895078524, 2.6443297528089991, 2.6332691065525093, 2.3990115849649438, 2.4559395053763788, 2.4025656987334716, 2.3211313627068777, 2.6916510582003332, 2.3127443867602566, 2.2275500362254843, 2.4586009397995547, 2.2697626991126034, 2.0742225924665028, 2.4157360988966654, 2.1756447895294135, 1.828184827763808, 2.1223009886451325, 2.1394544049563238, 1.7657685499751259, 1.8179879776100569, 1.7003112886930023, 1.7101462941150707, 1.738705954586423, 1.656335209644443, 1.4557393742484035, 1.4772364121229553, 1.4644262372913599, 1.3779738527874188, 1.3031022865505024, 1.2853649552705284, 1.3462184595988147, 1.1523417363750792, 1.1793963299634518, 1.1437005652811933, 1.1912996657061112, 1.1953654461960725, 1.1010112539255938, 1.055300552136456, 1.1249269128842834, 1.0110444039163149, 1.0184269678169653, 1.0264504972327131, 1.0386865660907723, 0.96471899028541641, 0.93018157477337171, 1.0024338919654858, 0.93934321641538066, 0.87221004322354012, 0.86409687184501627, 0.84585432679435602, 0.89988947930481678, 0.9824057853083179, 0.8257956228268496, 0.80439242538570366, 0.85406389677063921, 0.9010521151265084, 0.91355683036848345, 0.88882632052127752, 0.83443361496878177, 0.87696617459276438, 0.92113363521079694, 0.91927860856395982, 0.87374726817495352, 0.86367281218918612, 0.85031774720853104, 0.85230621781811999, 0.81436218804527549, 0.81503470693344493, 0.83442810774712861, 0.87198614204694913, 0.82088426546697857, 0.80650520558469219, 0.81123578807684016, 0.80423887585702303, 0.78685034571465573, 0.84079087609070813, 0.7924848155945392, 0.78718814569803974, 0.78420934791679109, 0.77404723071916548, 0.79668636826156736, 0.76753913552774244, 0.7898525783150705, 0.78070969987023364, 0.7688621868401464, 0.77668303239600267, 0.72221001614793623, 0.74073236432484268, 0.73349201149731647, 0.72416660912196262, 0.7604333279171922, 0.73183625461166768, 0.77182574963745987, 0.73115090611971834, 0.79066362729224926, 0.73909445727391587, 0.76964280439476351, 0.78710498385911376, 0.76112081341770998, 0.75274867135842394, 0.77951432821727173, 0.79936393650367998, 0.78681263657067047, 0.73847785491229812, 0.74717365186796814, 0.80461061567686132, 0.84118930536028769, 0.80857996596430248, 0.7652918693557399, 0.77603079312906464, 0.77278099877078432, 0.77078514843044355, 0.79723030960600272, 0.8203686304519523, 0.77744194047490989, 0.80987471179286286, 0.76978963550273005, 0.74247057320584076, 0.73118533074494108, 0.72811159084423727, 0.76962004827342101, 0.8031341911912192, 0.76639848426987256, 0.76173921538309608, 0.75115529727786001, 0.77257498920115231, 0.74242410491337718, 0.77116994123243532, 0.7467197853176849, 0.76382932557038119, 0.74015972404473751, 0.74228668999770908, 0.77485442989801545, 0.76289788018735094, 0.76372883280922987, 0.74491660551957739, 0.76716994787481729, 0.74280769656118673, 0.78861816511995086, 0.7622998420088678, 0.75761906169942184, 0.7930231978300939, 0.75108339941505931, 0.75145423018610868, 0.75930143994923793, 0.78677704121381176, 0.76996847065713636, 0.75140661212566096, 0.77283800722529539, 0.7735223818716388, 0.78240864082173589, 0.73937895875234849, 0.76659783520701263, 0.74932253019700412, 0.77447822999304305, 0.79629575882383297, 0.76613652441914581, 0.7712700999715969, 0.76685222217663718, 0.86448684808155518, 0.79763972173869035, 0.83286960568866419, 0.77552542011808845, 0.79122646648430428, 0.78085327927032089, 0.80284252730283046, 0.83923314803502513, 0.8442540510785117, 0.86755268923086504, 0.94100003849661418, 0.89666072818412235, 0.93060482260034383, 0.8855056682149306, 0.88643569757639074, 0.85773760668684496, 0.84654914848622875, 0.80834497292056751, 0.8014569380189781, 0.80586348516900863, 0.82885337928051561, 0.79042373579138392, 0.80670178266121773, 0.78015728125371819, 0.78880895822536434, 0.78172549584775408, 0.78496239185492933, 0.79391977421721738, 0.81545524583626361, 0.78233017841625629, 0.79026769998248902, 0.77973663165501994, 0.77326599815191255, 0.77869478361885169, 0.76663502667934125, 0.75417645975768777, 0.78272634886422809, 0.78388143935249432, 0.78965148999051271, 0.76989701260742427, 0.79780443686814029, 0.75746497939301582, 0.7760931881417813, 0.76579762417100827, 0.77091260879847212, 0.7693456075216093, 0.74561908490054418, 0.77551519770309441, 0.78581340181964121, 0.78355779326709152, 0.7699801308247548, 0.77253712816829545, 0.77897282079890084, 0.78076536327763024, 0.75981833260085074, 0.78218993407640425, 0.77607026482211283, 0.7889841573697538, 0.77096709212124903, 0.77027881105552398, 0.77103503346487423, 0.78389673250913261, 0.76277898383408949, 0.78668018999695999, 0.78826377550702709, 0.78763017881240094, 0.7913096472083454, 0.80161842804775307, 0.78630959788501786, 0.79802994534315697, 0.78096085382102809, 0.78060666076625917, 0.78902200792418486, 0.77451770334420567, 0.78824692186838663, 0.78659762315021187, 0.80000741259701069, 0.80165733026443775, 0.8045298381955116, 0.80112709196371379, 0.78292018392977936, 0.77949752642484105, 0.80416040581306159, 0.79557449110099954, 0.80081972986587069, 0.79084069319819494]
    a_x=np.arange(1,len(loss_list)+1)
    plt.plot(a_x,np.array(loss_list))
    plt.show()
# time_plot()
# dd=[[2,3],[4,5]]
# data=pd.DataFrame(dd,columns=[3,4],index=[1,2])
# a=np.array(data.columns)
# b=np.array(data.index)
# print(a.shape,b.shape)
# print(np.dot(a,b))

tt=np.arange(5)
bb=np.cos(np.pi*tt)
print(np.dot(tt,bb))

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

from sklearn.metrics import average_precision_score
# labels_all = np.array([2, 2, 3, 3, 5])
# preds_all = np.array([2, 2, 4, 3, 3])
# n=average_precision_score(labels_all,preds_all)
# print ('accuracy={0}'.format(n))
#accuracy =  sklearn.metrics.accuracy_score(labels_all, preds_all)
#f1 = 100 * sklearn.metrics.f1_score(labels_all, preds_all, average='weighted')
#print ('accuracy={0},f1={1}'.format(accuracy,f1))
#print('========pre_score===')
#print (pre_sore)
#false_posrate,ture_posrate,thresholds=roc_curve(labels_all,preds_all)
#print(false_posrate,ture_posrate,thresholds)

"""
        res={'zhangdan':{'bj':1,'age':12,'sex':1},'xunying':{'bj':2,'age':134,'sex':2},'zy':{'age':134,'sex':2}}

        data=pd.DataFrame(res)# 成绩矩阵，行表示用户，列表示课程
        print(data)
        data=pd.DataFrame.fillna(data,0)
        print(data)
        for i in range(10):
            self.hidden1_i = GraphConvolutionSparse(input_dim=self.input_dim,
                                                  output_dim=FLAGS.hidden1,
                                                   adj=self.adj[i],
                                                  features_nonzero=self.features_nonzero,
                                                  act=tf.nn.relu,
                                                  dropout=self.dropout,
                                                  logging=self.logging)(self.inputs)
            self.embeddings = GraphConvolution(input_dim=FLAGS.hidden1,
                                               output_dim=FLAGS.hidden2,
                                               adj=self.adj[i],
                                               act=lambda x: x,
                                               dropout=self.dropout,
                                               logging=self.logging)(self.hidden1_i)
            self.z_mean = self.embeddings+self.z_mean
"""